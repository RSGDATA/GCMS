# Snowflake Daily Pipeline Cost Analysis
**Date:** February 6, 2026  
**Project:** GCMS Data Platform   
**Scenario:** Optimized Production Pipeline Running Daily

---

## üìä Executive Summary

**Estimated Monthly Cost for Daily Pipeline: $6-9/month**

With optimizations applied:
- Daily dbt runs: ~$0.20-0.30/day
- Monthly total: ~$6-9
- Well within 10 credit limit
- **Safe for production use**

---

## üíµ Credit-to-Dollar Conversion (X-SMALL Warehouse)

**Current Configuration:** X-SMALL Warehouse

| Metric | Value |
|--------|-------|
| **Warehouse Size** | X-SMALL |
| **Credits per Hour** | 1 credit |
| **Cost per Credit** | $3.00 (Standard On-Demand) |
| **Cost per Hour** | $3.00 |
| **Cost per Minute** | $0.05 |
| **Cost per Second** | $0.00083 |

### Quick Reference Table

| Runtime | Credits | Cost |
|---------|---------|------|
| 1 minute | 0.0167 | $0.05 |
| 5 minutes | 0.0833 | $0.25 |
| 10 minutes | 0.1667 | $0.50 |
| 30 minutes | 0.5000 | $1.50 |
| 1 hour | 1.0000 | $3.00 |
| 1 day (24 hrs) | 24.0000 | $72.00 |

**Note:** With 60-second auto-suspend, warehouses only run when actively processing queries, minimizing idle costs.

---

## üí∞ Daily Pipeline Cost Projection

### Pipeline Components

#### 1. Data Ingestion (Airbyte/Manual)
**Frequency:** Once daily  
**Warehouse:** AIRBYTE_WAREHOUSE (X-SMALL)  
**Duration:** ~2-3 minutes  
**Operations:**
- Insert into RAW tables (9 tables)
- ~50-100 rows total per day

**Cost Calculation:**
- Warehouse: X-SMALL = 1 credit/hour
- Runtime: 3 minutes = 0.05 hours
- Cost per run: 0.05 credits
- **Daily cost: $0.15** (at $3/credit)

---

#### 2. dbt Transformation (Bronze ‚Üí Silver ‚Üí Gold)
**Frequency:** Once daily  
**Warehouse:** TRANSFORMING (X-SMALL)  
**Duration:** ~5-8 minutes  
**Operations:**
- 13 Bronze models (br_supabase_*)
- 3 SCD2 models (br_*_scd2)
- 4 Silver staging models (stg_*)
- 3 Silver intermediate models (int_*)
- 3 Gold models (dim_*, fct_*)

**Cost Calculation:**
- Warehouse: X-SMALL = 1 credit/hour
- Runtime: 8 minutes = 0.133 hours
- Cost per run: 0.133 credits
- **Daily cost: $0.40** (at $3/credit)

---

#### 3. Data Quality Checks
**Frequency:** Once daily  
**Warehouse:** TRANSFORMING (X-SMALL)  
**Duration:** ~1 minute  
**Operations:**
- Row count validation
- Schema checks
- Data freshness tests

**Cost Calculation:**
- Runtime: 1 minute = 0.017 hours
- Cost per run: 0.017 credits
- **Daily cost: $0.05** (at $3/credit)

---

### Total Daily Pipeline Cost

| Component | Credits | Cost (@ $3/credit) |
|-----------|---------|-------------------|
| Data Ingestion | 0.05 | $0.15 |
| dbt Transformation | 0.133 | $0.40 |
| Data Quality | 0.017 | $0.05 |
| **Total per Day** | **0.20** | **$0.60** |

---

## üìÖ Monthly Cost Projections

### Scenario 1: Daily Pipeline Only

**Assumptions:**
- Pipeline runs once per day
- No manual queries
- No Streamlit usage

| Metric | Value |
|--------|-------|
| Days per month | 30 |
| Credits per day | 0.20 |
| **Monthly credits** | **6.0** |
| **Monthly cost** | **$18** |

**Status:** ‚úÖ Within 10 credit limit (60% usage)

---

### Scenario 2: Daily Pipeline + Light Development

**Assumptions:**
- Pipeline runs once per day
- 2-3 manual dbt runs per week for testing
- Streamlit used 2 hours per week

| Component | Credits/Month |
|-----------|---------------|
| Daily pipeline | 6.0 |
| Manual dbt runs | 0.5 |
| Streamlit queries | 0.3 |
| **Total** | **6.8** |
| **Monthly cost** | **$20.40** |

**Status:** ‚úÖ Within 10 credit limit (68% usage)

---

### Scenario 3: Daily Pipeline + Moderate Development

**Assumptions:**
- Pipeline runs once per day
- Daily manual testing/development
- Streamlit used 5 hours per week

| Component | Credits/Month |
|-----------|---------------|
| Daily pipeline | 6.0 |
| Development work | 1.5 |
| Streamlit queries | 0.8 |
| **Total** | **8.3** |
| **Monthly cost** | **$24.90** |

**Status:** ‚úÖ Within 10 credit limit (83% usage)  
**Alert:** Will trigger 75% warning

---

### Scenario 4: Maximum Safe Usage

**Assumptions:**
- Pipeline runs once per day
- Heavy development activity
- Streamlit used extensively

| Component | Credits/Month |
|-----------|---------------|
| Daily pipeline | 6.0 |
| Development work | 2.5 |
| Streamlit queries | 1.5 |
| **Total** | **10.0** |
| **Monthly cost** | **$30** |

**Status:** ‚ö†Ô∏è At 10 credit limit (100% usage)  
**Alert:** Will trigger suspension at 90%

---

## üìà Cost Optimization Strategies

### Already Implemented ‚úÖ

1. **X-SMALL Warehouses** - Minimum cost per hour
2. **60-Second Auto-Suspend** - No idle time waste
3. **Resource Monitor** - Hard limit at 10 credits
4. **Efficient Queries** - All queries < 0.001 credits

### Additional Optimizations

#### 1. Incremental dbt Models
**Current:** Full table refreshes  
**Optimized:** Incremental updates

```sql
{{ config(
    materialized='incremental',
    unique_key='id'
) }}
```

**Impact:** 30-50% reduction in dbt runtime  
**Savings:** $0.12-0.20/day = $3.60-6.00/month

---

#### 2. Query Result Caching
**Current:** Each query executes  
**Optimized:** Reuse cached results (24 hours)

**Impact:** 20-30% reduction in repeated queries  
**Savings:** $0.06-0.12/day = $1.80-3.60/month

---

#### 3. Streamlit Query Caching
**Current:** Queries on every page load  
**Optimized:** Cache for 1 hour

```python
@st.cache_data(ttl=3600)
def get_concerts():
    # ... query
```

**Impact:** 80-90% reduction in Streamlit queries  
**Savings:** $0.24-0.40/day = $7.20-12.00/month

---

## üéØ Recommended Production Configuration

### Pipeline Schedule

**Optimal:** Once daily at 2 AM
```python
# In Airflow DAG
schedule_interval='0 2 * * *'  # 2 AM daily
```

**Why:**
- Off-peak hours (if applicable)
- Consistent timing
- Allows for overnight data collection

---

### Resource Allocation

| Warehouse | Size | Auto-Suspend | Use Case |
|-----------|------|--------------|----------|
| TRANSFORMING | X-SMALL | 60s | dbt, queries |
| AIRBYTE_WAREHOUSE | X-SMALL | 60s | Data ingestion |
| COMPUTE_WH | X-SMALL | 60s | Ad-hoc queries |

---

### Monitoring Thresholds

| Threshold | Action |
|-----------|--------|
| 5 credits | Normal - on track |
| 7.5 credits (75%) | Warning email - review usage |
| 9 credits (90%) | Critical - warehouses suspend |
| 10 credits (100%) | Hard stop - immediate suspension |

---

## üìä Cost Comparison: Daily vs. On-Demand

### Daily Scheduled Pipeline

| Metric | Value |
|--------|-------|
| Runs per month | 30 |
| Credits per month | 6.0 |
| Cost per month | $18 |
| Predictability | High ‚úÖ |
| Automation | Full ‚úÖ |

---

### On-Demand Pipeline (Current)

| Metric | Value |
|--------|-------|
| Runs per month | Variable (10-20) |
| Credits per month | 4-8 |
| Cost per month | $12-24 |
| Predictability | Low ‚ö†Ô∏è |
| Automation | Manual ‚ö†Ô∏è |

**Recommendation:** Daily pipeline provides better value through automation and predictability.

---

## üí° Cost-Saving Best Practices

### Development Environment

1. **Use Separate Dev Warehouse**
   - Create DEV_WH with same settings
   - Separate from production resource monitor
   - Allows testing without affecting production limits

2. **Limit Streamlit Usage**
   - Stop when not actively using
   - Implement query caching
   - Use sample data for development

3. **Optimize dbt Development**
   - Use `dbt run --select model_name` for single models
   - Leverage `dbt build --select state:modified+` for changed models only
   - Don't run full pipeline during development

---

### Production Environment

1. **Schedule Wisely**
   - Run once daily (not hourly)
   - Off-peak hours if applicable
   - Consolidate multiple runs

2. **Monitor Actively**
   - Weekly cost checks
   - Review query performance
   - Identify expensive queries

3. **Scale Appropriately**
   - Start with X-SMALL
   - Only increase if performance issues
   - Monitor query execution times

---

## üîÆ Future Scaling Considerations

### When to Increase Resources

**Indicators:**
- Query execution > 5 minutes consistently
- Data volume > 1 GB
- More than 100 dbt models
- Multiple concurrent users

**Scaling Options:**
1. Increase warehouse size to SMALL (2 credits/hour)
2. Increase resource monitor limit to 20 credits
3. Implement query optimization first

---

### Cost at Scale

| Warehouse Size | Cost/Hour | Daily Pipeline | Monthly Cost |
|----------------|-----------|----------------|--------------|
| X-SMALL (current) | $3 | $0.60 | $18 |
| SMALL | $6 | $1.20 | $36 |
| MEDIUM | $12 | $2.40 | $72 |

**Recommendation:** Stay on X-SMALL until data volume requires scaling.

---

## üìã Action Items

### Immediate (Already Done ‚úÖ)
- [x] Set warehouses to X-SMALL
- [x] Configure 60-second auto-suspend
- [x] Create 10 credit resource monitor
- [x] Apply resource monitor to all warehouses

### Short-term (Next 2 Weeks)
- [ ] Implement incremental dbt models
- [ ] Add Streamlit query caching
- [ ] Set up Airflow daily schedule
- [ ] Configure email alerts for resource monitor

### Long-term (Next Month)
- [ ] Review query performance monthly
- [ ] Optimize slow-running queries
- [ ] Consider separate dev/prod warehouses
- [ ] Evaluate scaling needs

---

## üìû Cost Monitoring Commands

### Daily Cost Check
```bash
python3 -c "
import snowflake.connector
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization

with open('.snowflake/snowflake_key', 'rb') as f:
    p_key = serialization.load_pem_private_key(f.read(), password=None, backend=default_backend())
pkb = p_key.private_bytes(encoding=serialization.Encoding.DER, format=serialization.PrivateFormat.PKCS8, encryption_algorithm=serialization.NoEncryption())

conn = snowflake.connector.connect(user='ROBERTGONZALEZ', account='hndffbt-te29703', private_key=pkb, role='ACCOUNTADMIN')
cursor = conn.cursor()
cursor.execute('SELECT SUM(credits_used) FROM snowflake.account_usage.warehouse_metering_history WHERE start_time >= CURRENT_DATE')
print(f'Today: {cursor.fetchone()[0] or 0:.3f} credits')
conn.close()
"
```

### Monthly Projection
```bash
python3 -c "
import snowflake.connector
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization

with open('.snowflake/snowflake_key', 'rb') as f:
    p_key = serialization.load_pem_private_key(f.read(), password=None, backend=default_backend())
pkb = p_key.private_bytes(encoding=serialization.Encoding.DER, format=serialization.PrivateFormat.PKCS8, encryption_algorithm=serialization.NoEncryption())

conn = snowflake.connector.connect(user='ROBERTGONZALEZ', account='hndffbt-te29703', private_key=pkb, role='ACCOUNTADMIN')
cursor = conn.cursor()
cursor.execute('SELECT SUM(credits_used) FROM snowflake.account_usage.warehouse_metering_history WHERE start_time >= DATE_TRUNC(month, CURRENT_DATE)')
monthly = cursor.fetchone()[0] or 0
print(f'Month-to-date: {monthly:.2f} credits ({monthly/10*100:.1f}% of limit)')
conn.close()
"
```

---

## ‚úÖ Conclusion

### Summary

**Daily Pipeline is Cost-Effective:**
- Estimated $18/month for automated daily runs
- Well within 10 credit ($30) limit
- Provides consistent, reliable data updates
- Better than manual on-demand approach

**With Optimizations:**
- Can reduce to $12-15/month
- Allows room for development work
- Maintains data freshness
- Predictable costs

**Recommendation:** ‚úÖ **Proceed with daily pipeline**

The optimized Snowflake configuration supports a daily production pipeline while staying well within budget constraints.

---

**Analysis Date:** February 6, 2026  
**Next Review:** March 6, 2026  
**Status:** ‚úÖ Ready for Production
